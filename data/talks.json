{
  "contentaware-playback-for-lowlatency-live-streaming-of-sports": {
    "title": "Content-Aware Playback for Low-Latency Live Streaming of Sports",
    "description": "There are two main factors that determine the viewer experience during the live streaming of sports: latency and stalls. Latency should be low and stalls should not occur. Yet, these two factors work against each other and it is not trivial to strike the best trade-off between them. One of the best tools we have today to manage this trade-off is the adaptive playback speed control. This tool allows the streaming client to slow down the playback when there is a risk of stalling and increase the playback when there is no risk of stalling but the live latency is higher than desired. While adaptive playback generally works well, the artifacts due to the changes in the playback speed should preferably be unnoticeable to the viewers. However, this mostly depends on the part of the audio/video content subject to the playback speed change. In this talk, we discuss the details of a content-aware playback speed control (CAPSC) algorithm we developed for dash.js along with the metadata we defined to indicate the event densities in a given content. CAPSC aims to keep the playback speed close to the nominal speed (1x) during the important parts of the content and gracefully adapts the playback speed changes to provide a more pleasant viewing experience. Our ultimate goal is to apply CAPSC on live soccer games in real time through a third-party company (e.g., Wyscout, TheSports, and InStat) that provides real-time game data and statistics. ",
    "speakers": [
      "Ali C. Begen"
    ]
  },
  "ocf-at-nflx": {
    "title": "OCF at NFLX",
    "description": "Obtuse Crypto-Fish? Organic Chemotaxonomist’s Federation? Obvious Chafing Factor?<br><br>In the filmmaking process and within Netflix’s globally distributed collaboration environments, “OCF” refers to Original Camera Files (or Original Capture Formats, depending on who you ask). Call them what you like, but their importance cannot be overstated. They contain the data captured by a camera sensor, and represent the digital equivalent of a negative in film-based workflows of yore. Post-production processes like review, editorial and visual effects cannot begin without OCF.<br><br>This presentation will outline the unique challenges associated with managing digital camera files in the course of content creation and demonstrate how we bring joy to both our members and creative partners alike as we scale best practices of content production, leverage open standards like OpenTimelineIO and ACES, and implement new tools tailored to the specific needs of filmmakers. These solutions include fast and intelligent transfer of multi-terabyte camera cards, classification of shots and their relationships to the final cut, and media optimization for use throughout a complex production pipeline.<br>",
    "speakers": [
      "Matthew Donato"
    ]
  },
  "dos-and-donts-in-producing-a-vr-live-stream": {
    "title": "Dos and don&#39;ts in producing a VR live stream",
    "description": "VR may sound like a gimmick at the moment, but there is no denying in its potential. To prepare for the future, the Evolution recently launched a live VR game. There are not many guides out there on how to produce a proper live VR stream, so we had to experiment and find out the ways ourselves. The purpose of this talk is to share some points we learned on dos and don&#39;ts of VR live production that could help producing a better live VR experience.",
    "speakers": [
      "Behnam Kakavand"
    ]
  },
  "accessibility--media-players": {
    "title": "Accessibility &amp; Media Players",
    "description": "Oftentimes, when people bemoan the state of accessibility for media playback, they’re talking about the media content itself: closed captioning, audio descriptions, available transcripts, seizure-causing flashes in video content, scene color contrast, and the like. These are unquestionably important considerations when attempting to making your users’ media playback experience accessible, but they are unfortunately also very hard and sometimes quite costly problems. Not only that, oftentimes these (at least currently) require considerations and buy-in at the production phases of content creation, well before the media makes its way into streaming media pipelines, platforms, and players.<br><br>Yet there is an area of much easier, cheaper, and tractable improvements to accessibility available to video engineers - the media player itself. In this talk, I’ll give an overview of the primary considerations for making an accessible browser-based media player. At its core, good media player accessibility requires a semantics that is consistent and make sense for both non-technical users and video developers alike. This goes hand in hand with coming up with the right metaphors for the different kinds of interactions and information surfacing—analogous to the physical metaphors at play for visual user experience. With these, we should be able to establish some best practices and reference implementations for common use cases. Finally, I’ll end by demo-ing a player-agnostic example implementation aiming toward these goals, along with some bonus wins you get for free when you make your media player accessible.",
    "speakers": [
      "Christian Pillsbury"
    ]
  },
  "puppeteer--canvas--webcodecs-to-replace-ffmpeg": {
    "title": "Puppeteer + &lt;canvas&gt; + WebCodecs to replace ffmpeg",
    "description": "We have a complex rendering pipeline that requires temporal, and graphical editing. Historically we&#39;ve used ffmpeg and it&#39;s filter graphs as our rendering engine.<br><br>However this has lead to duplicate work in our user interface to preview rendered output. We have invested time in understanding the demux/decode/filter/encode/mux pipeline.<br><br>We have come up with techniques combining Puppeteer, MP4Box.js, WebCodecs, &lt;canvas&gt;, and ffmpeg (final muxing) together to create a pipeline that gives web developers a familiar &lt;canvas&gt; graphics API, without sacrificing the performance characteristics of ffmpeg.",
    "speakers": [
      "Collin Miller"
    ]
  },
  "content-classification-for-live-streams-with-ffmpeg": {
    "title": "Content Classification for Live Streams with ffmpeg",
    "description": "ffmpeg is mostly known for being the swiss army knife of video processing.  In recent years, as the advancement of AI-based video processing starts to become popular, ffmpeg has introduced many of these functionalities through a number of filters.  These filters has dramatically lowered the technical barrier for doing AI-based video processing.<br><br>In this talk, we&#39;ll examine the various existing filters like SuperResolution or Derain.  We will also talk about our experience creating a scene classification filter - training the models, implementing the filter using ffmpeg&#39;s tensorflow backend, and using GPUs to run the model for live streams.  This filter is open source, and currently detects content types such as professional soccer games or adult content.  We will share benchmarks and our learnings in tuning the performance for this filter.",
    "speakers": [
      "Eric Tang"
    ]
  },
  "open-caching-cdni-metadata-model-extensions": {
    "title": "Open Caching: CDNI Metadata Model Extensions",
    "description": "The interchange of content delivery configuration metadata between the various entities in the delivery ecosystem is essential for efficient interoperability. The need for an industry-standard API and metadata model becomes increasingly important as content and service providers automate more of their operations, and as technologies, such as open caching, require coordination of content delivery configurations. This talk discusses the Open Caching configuration interface, how the project was born, how it is moving through the SVA  and IETF, and what it means for the industry.",
    "speakers": [
      "Glenn Goldstein"
    ]
  },
  "improving-streaming-experience-with-bayesian-optimization-from-ab-to-az-test-": {
    "title": "Improving Streaming Experience with Bayesian Optimization, from AB to AZ test !",
    "description": "The video streaming system of Netflix has hundreds of configuration parameters that influence many aspects of the playback behavior when using our service; for example, such configurations specify the amount of video content to load before we begin playback to balance play delay and risk of rebuffers.<br>Usually, we perform many iterations of A/B experiments to fine-tune these values and provide the best possible member experience across the wide range of platforms and networks we serve worldwide. Still, identifying good configurations that work well across diverse networks and devices, in particular when dealing with multi-dimensional parameters, is challenging given their complex interactions with various streaming metrics.<br>To help with these challenges, one powerful approach we have evaluated in the last years is Bayesian optimization. With this method, we can efficiently explore and understand the relationship between configuration parameters and objective metrics (such as playdelay, rebuffer rate, …)  by building a surrogate model that incorporates experimental observations and guides future experiments.<br><br>In this presentation, we give a brief introduction into the world’s leading streaming entertainment service Netflix - followed by an example use case on how we use Bayesian optimization in conjunction with our A/B experimentation framework to deliver concrete service improvements for our users.",
    "speakers": [
      "Guillaume du Pontavice"
    ]
  },
  "the-browser-as-a-video-rendering-engine-deep-dive-into-time-management": {
    "title": "The browser as a video rendering engine: Deep dive into time management",
    "description": "The software I work on allows users to design video clips in the browser. Users can combine video clips with captions and other elements into multiple scenes, which we render to a rasterised video.<br><br>This means we need to keep different forms of playable media in sync over time, while playing, seeking, and scrubbing in the designer UI, as well as during rendering.<br><br>My talk will cover how we implemented our time state tracking in React, including:<br>• Embracing the concept of &quot;derived state&quot; for reliable, deterministic rendering<br>• Optimising performance through various techniques<br>• How to test time-based state (or, how to time-travel in your tests)<br>• How to sync various types of media (videos, captions, etc) with a single source of truth<br><br>This will help anyone, who wants to build a video editor and/or rendering system in the browser, set up a solid foundation for handling time in their UI.",
    "speakers": [
      "Jacques Blom"
    ]
  },
  "live-video-amas-in-30-days": {
    "title": "Live Video AMAs in 30 Days",
    "description": "Apollo 350 built an in-browser platform for a startup to support live video AMA (ask me anything) events. We needed to support thousands of viewers and multiple video conference hosts, where the hosts could talk in real-time to each other. We wanted a solution that would allow us to customize the video that was available for viewers. In our case, we needed to shape the video of each host into a circle and animate them larger/smaller based on who was speaking. We also did not want to rely on any processing on the &quot;hosts&#39;&quot; side due to host browser limitations. The &quot;hosts&quot; would simply log into the website and be able to broadcast from their browser (desktop or mobile).<br><br>We had to build this quickly and needed to leverage existing services like Twilio, but we also built some custom solutions of our own. We&#39;ll talk about the process of how we got to a working product and the decisions (both good and bad) that we had to make. We will share the architecture we ended up with.<br><br>Apollo 350 is a software consulting company that has extensive experience building and launching video products to millions of users with billions of views. We consist of tech executives and leads who launched VEVO, Condé Nast Entertainment, and LinkedIn Video.",
    "speakers": [
      "Janet Kim"
    ]
  },
  "updates-on-the-ffmpeg-and-free-software-community": {
    "title": "Updates on the FFmpeg and Free software community",
    "description": "I plan to talk about the changes in the FFmpeg community, (in the libav community), and what happened in the FFmpeg project - code wise - and our realization. FFmpeg 5.0, releases, CoC and a few other things. I plan to speak about dav1d, x264/rav1e on ARM too.",
    "speakers": [
      "Jean-Baptiste Kempf"
    ]
  },
  "distributed-request-tracing-for-the-streaming-video-ecosystem": {
    "title": "Distributed Request Tracing for the Streaming Video Ecosystem",
    "description": "Content providers, who frequently rely on third party software and services (Players, CDNs, Origin Services), struggle to develop the observability necessary to achieve their QoE goals. They are essentially responsible for the entire customer experience but only able to fully observe what they themselves instrument and what their service providers are able/willing to share. In recent years, there has been some progress along these lines in the form of CMCD-based data propagation of player/app metrics and CDN log streaming but logs and metrics are still insufficient for enabling the deep observability necessary for consistently amazing user experiences. <br><br>In addition, 3rd party service providers (e.g. CDNs, origin services, packages, etc) struggle to provide optimal service to their customers (e.g. content publishers) due to the same observability challenge. In short, telemetry is fragmented and siloed making it virtually impossible to get the complete architectural or operational picture. <br><br>As a result of these conditions the Steaming Video Alliance&#39;s QoE working group is developing methods for augmenting logs and metrics with the third pillar of observability, distributed request tracing. Typically distributed request tracing is performed using standardized methods based on the open telemetry project in the context of a single service architecture. Our project is designed to span 3rd party services across the video ecosystem from player to CDN to Origin and beyond using logging mechanisms already in place.<br><br>In this talk we&#39;ll cover the methods for implementing our first phase of this initiative from player to Origin and back. We&#39;ll talk about early results and how tracing can be leveraged to turn high level QoE warning signals into deep root cause analysis. If successful this effort promises to enable game-changing quality for not only content publishers but the 3rd party services that support them.",
    "speakers": [
      "Josh Evans"
    ]
  },
  "understanding-typography-video-rendering--through-captions": {
    "title": "Understanding typography video rendering - through captions",
    "description": "I worked on Google Fonts and have since built a startup that focuses on rendering out video content based on a novel way of composing HTML content. In my experience working on Google Fonts, I learned about the history of fonts, glyphs, and the modern challenges around rendering text. This talk will deep dive into various aspects of digital typography, and also discuss interesting factors as it adheres to rasterized fonts. I will also dig into burned typography (After Effects, 3D graphics), captions, and even touch on VR. <br><br>I will start with talking about the digital typography field and how it all started with GUIs in the 80s. The decisions made to translate vector type to pixel form has been a precursor to nearly all text rendering in video. I will start from the early font deals Steve Jobs made and discuss Adobe&#39;s role in typography and printing. This will touch on things like vectors, kerning, and how fonts are made.<br><br>Then I will talk about how film has depicted text and fonts over the past 100 years, and discuss the transition that occurred when digital typography took over. I need to research this, but I&#39;m sure its interesting and I can do a lot of visual stuff to make it engaging.<br><br>Then I&#39;ll go into how digital typography has evolved from flat text to 3D rendered text. This section will discuss the innovations around 3d rendering and how the ability to translate text (the same digital text from the 80s) has become distinct with the modern 3d rendering stack. I&#39;ll do a little research on this by talking with effects producers, but I have a clear group of primary sources from Amazon Prime&#39;s post-production staff.<br><br>Finally, I&#39;ll touch on things like captions (as its currently working) and VR, and explain how most of the most interesting fields are limited based on some early decisions that were made.",
    "speakers": [
      "Leonard Bogdonoff"
    ]
  },
  "how-we-accelerated-live-2-vod-delivery-from-15-minutes-to-under-a-minute": {
    "title": "How we accelerated Live 2 VOD delivery from 15 minutes to under a minute",
    "description": "In 2020 we decided to replace our aging Live 2 VOD system with a new system that would hopefully fix some of the issues with the system already in place, such as missing signaling about programming, having to maintain a separate L2V origin forever and lack of frame accuracy. We wanted to create a system that would run completely automated based on our playout systems schedule, requiring no manual intervention or manual cutting. We also wanted the system to allow users to immediately play content as soon as it was broadcast, with the option to extend to &quot;start over&quot; functionality later. Lastly, we wanted Live 2 VOD videos to be served from our normal VOD library.<br><br>The talk covers:<br>How we found a way to integrate with traditional broadcast systems to get highly accurate and automated VOD assets from traditional broadcast channels with systems already on place for 31 channels (8 national, 8 regional and 15 events)<br><br>How we process 250 transmissions per day, of which ~40 are published to the service with a length of 5 mins to 8 hrs.<br>How we managed to wrangle the existing playout system to provide usable SCTE-104 insertion that we could convert to SCTE-35 and later read.<br><br>How we combined using a live buffer with producing VOD assets to simplify long term storage of captured VOD assets and avoid having a dedicated L2V origin while still having content available in under 1 minute.<br><br>How we handle situations with sports that have a break between multiple parts.<br><br>Why it is sometimes impossible to get accurate and automated markers within the reality of a broadcast world.<br>How we had a manual handling rate of less than 1% while handling content for Wimbledon, Tour de France, several ATP tournaments, and the Handball World Championship 2021.<br>You should pick this talk because it covers the interaction between the traditional broadcast domain and streaming services, this wasn&#39;t really covered much in the 2020 Demuxed talks. We take a look at a challenge and a infrastructure landscape that is likely to be something many broadcasters share as they work to move focus to streaming services and internet delivery while customers continue to cable cut or cable shed.",
    "speakers": [
      "Loke Dupont"
    ]
  },
  "lowlatency-video-over-quic": {
    "title": "Low-Latency Video over QUIC",
    "description": "QUIC (RFC 9000) is a new network protocol designed to power HTTP/3, but it&#39;s also a powerful transport for other applications like video. There are multiple approaches for mapping video to the QUIC API, varying based on the target latency and user experience. At Twitch/IVS we&#39;ve built a new distribution protocol (Warp) to replace our HLS stack, utilizing a unique prioritization scheme to minimize latency in the face of congestion.",
    "speakers": [
      "Luke Curley"
    ]
  },
  "how-to-build-a-5day-antipiracy-solution-for-a-live-streaming-sport-event": {
    "title": "How to build a 5-day anti-piracy solution for a live streaming sport event",
    "description": "On a quiet winter evening, a company called and told us: &quot;Last weekend we streamed a Pay Per View soccer match in Peru and we were pirated through Twitch, Facebook and YouTube. The next match is in 5 days. Can you help us?&quot;<br><br>This talk will cover the challenges we faced and the strategies we used to be able to get to production a solution that we built in just 5 days.<br><br>The talk will show the architecture, which was created using:<br>- AWS MediaLive for live transcoding.<br>- AWS MediaPackage to encrypt and produce a DASH rendition.<br>- AWS CloudFront for content distribution.<br>- VideoJS as player.<br>- Our client side fingerprinting plugin for VideoJS.<br>- Our small backend and key server, that uses CENC ClearKey and that is able to ban users.<br>- Our web interface to ban pirate&#39;s sessions.<br><br>Finally, we will share the fun we had in searching and hunting pirates during a live soccer match.",
    "speakers": [
      "Nicolas Levy"
    ]
  },
  "the-mostly-definitive-history-of-smellovision": {
    "title": "The mostly definitive history of Smell-O-Vision",
    "description": "While idly scrolling through my Twitter feed one evening, I stumbled on a Tweet containing a video of Matt and Phil trying their absolute hardest to make us submit talk submissions before the last moment humanly possible.<br><br>As usual, I laughed in derision, and reached out with my oversized thumb to scroll away, but then Phil said something that stopped me in my tracks.<br><br>&quot;Smell-o-vision&quot;, he said. Could it be that simple? Could this be my golden ticket to the Demuxed hall of fame?<br><br>So I started down the strangest, smelliest rabbit hole imaginable, researching the complete history of smell-o-vision, and let me tell you, my eBay recommendations will never be the same again.<br><br>This tongue-in-cheek talk is a whistle-stop tour of all things olfactory in the video technology space. We&#39;ll start in the early 1900s, with movie theatre owners selecting and blowing their own scents into the theatre, in many cases before the films even had audio tracks, traveling through to smell-o-vision and the competitive AromaRama standard wars (Spoiler: they both turned out to be betamax).<br><br>Next we&#39;ll jaunt through the deeply questionable history of &quot;scratch-and-sniff&quot; cards, both in the movie theatre, but also shoved in a DVD case for you to enjoy in your own home, culminating in a live, on stage, scratching and sniffing of a 40 year old &quot;ODORAMA&quot; card, with second opinion from Matt McClure himself.<br><br>Penultimately, we&#39;ll take an ill-advised detour into the noxious dark ages of smell-enabled VR headsets with a very unsocially distanced look at the Nosulus Rift (the only device ever designed to emit a custom Fartgrance), and ponder if any of the upcoming &quot;Digital scent technology&quot; devices on Indiegogo are actually worth buying.<br><br>Finally, we&#39;ll arrive in 2021 and review what&#39;s to become the future of streaming - O(dor)TT. It might surprise you to hear that there isn&#39;t currently a standard for delivering smells via HLS or DASH manifests, so first up, we&#39;ll fix that, and only then can we take a scene from one of the terrible scratch and sniff movies I bought, and wire up my very own home made smell-o-vision device. Join me, live on stage for the world&#39;s first demonstration of `EXT-X-SMELL` in action.<br><br>(This is not a joke, you can check my eBay history if you don&#39;t believe me...)",
    "speakers": [
      "Phil Cluff"
    ]
  },
  "txt2vid-ultralow-bitrate-compression-of-talkinghead-videos-via-text": {
    "title": "Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text",
    "description": "tl;dr:<br><br>We ask the question:<br>“Can we compress AV content generated via webcams to just text and recover videos with similar Quality-of-Experience compared to standard codecs in a low bitrate regime?”<br>and answer it in affirmative using state-of-the-art deep learning models.<br><br>Long Version:<br><br>Video represents the majority of internet traffic today leading to a continuous technological arms race between generating higher quality content, transmitting larger file sizes and supporting network infrastructure. Adding to this is the recent COVID-19 pandemic fueled surge in the use of video conferencing tools. Since videos take up substantial bandwidth (~100 Kbps to few Mbps), improved video compression can have a substantial impact on billions of people in developing countries or other locations with limited or unreliable broadband connectivity. Moreover, a reduction in required bandwidth can have a significant impact on global network performance by decreasing the network load for live and pre-recorded content, providing broader access to multimedia content worldwide. In this talk, we present a novel video compression pipeline, called Txt2Vid, which substantially reduces data transmission rates by compressing webcam videos (&quot;talking-head videos&quot;) to a text transcript. The text is transmitted and decoded into a realistic reconstruction of the original video using recent advances in deep learning based voice cloning and lip syncing models. Our generative pipeline achieves two to three orders of magnitude reduction in the bitrate as compared to the standard audio-video codecs, while maintaining equivalent Quality-of-Experience based on a subjective evaluation by users (n=242) in an online study. The code for this work is available as an open-source project on GitHub (https://github.com/tpulkit/txt2vid.git).<br><br>The focus of our work is on audio-video (AV) content transmitted from webcams during video conferencing or webinars. Current compression codecs (such as H.264 or AV1 for videos, and AAC for audio) lossily compress the input AV content by discarding details that have the least impact on user experience. However, the distortion measures targeted by these codecs are often low-level and attempt to penalize deviation from the original pixel values, or audio samples. But in reality, what matters most is the final quality-of-experience (QoE) when this media stream is shown to a human end-consumer. Thus, in our proposed pipeline, instead of working with pixel-wise fidelity metrics we directly approximate the original content such that the QoE is maintained. Compressing to text, we can achieve bitrates of ~100bps at similar QoE compared to a standard codec. The pipeline uses a state-of-the-art voice cloning model to convert text-to-speech (TTS), and a lip-syncing model to convert audio to reconstructed video using a driving video at the decoder. Our pipeline can be used for storing the webcam AV content as a text file or for streaming this content on the fly. We evaluated our pipeline using a subjective study on Amazon Mturk to compare user preferences between Txt2Vid generated videos and videos compressed with standard codecs at varying levels of compression, for multiple contents.<br><br>The Sell:<br><br>We believe the proposed framework has the potential to change the landscape of video storage and streaming. It can enable several applications with great potential for social good expanding the reach of video communication technology. Some examples include better accessibility in areas with poor internet availability, transmission of pedagogical content for remote learning, real-time machine translation of talks. It can also enable some fun applications such as joining a AV call but just typing in your input instead of speaking.  <br><br>While we used specific tools in our pipeline to demonstrate its capabilities, we envision significant progress in the components used over the coming years. In particular, we would like to highlight that this is just a prototype for streaming content, and a perfect timing for the community to be involved to make it more practical and accessible. Potential improvements to the current framework and implementation include reducing computational complexity, higher latency for streaming, improved Quality-of-Experience to include more non-verbal cues, and ethical concerns over usage of such a technology. We call upon the community to improve and build upon this framework and adapt it for different applications.<br><br><br><br><br><br><br>",
    "speakers": [
      "Pulkit Tandon"
    ]
  },
  "using-the-power-of-edge-compute-to-enhance-streaming-delivery": {
    "title": "Using The Power of Edge Compute To Enhance Streaming Delivery",
    "description": "Streaming media formats are continually being updated with new features. Additionally, some platforms and rights holders are starting to require certain features be implemented/adopted.<br><br>In the past, the solution to this problem has been to re-encode and/or remux your existing media library to add the new features. This is expensive, time consuming and at times requires re-architecting your encoding/muxing pipeline to accommodate.<br><br>This talk will delve into the ins-and-outs of using Edge Compute platforms across multiple vendors to implement new features to existing media streams with a just-in-time and globally scaleable approach.<br><br>There will be demos showing features added to existing HLS streams:<br>- Adding Roku&#39;s JPEG based Trick Play<br>- Convert MPEG-TS based HLS to fMP4 based HLS<br>- Applying Encryption and DRM to Clear streams <br>- CDN Pre-Fetching/Per-Warming",
    "speakers": [
      "Robert Labonte"
    ]
  },
  "its-time-to-whip-webrtc-into-shape": {
    "title": "It&#39;s time to WHIP WebRTC into shape",
    "description": "WHIP WebRTC, into shape<br>Try to detect it, it&#39;s not too late<br>It&#39;s time to WHIP it, WHIP it good.<br><br>For many in broadcast and streaming, WebRTC is not “complete”, as it lacks a standard signaling protocol to make it work like RTMP or RTSP.<br><br>WHIP, the WebRTC HTTP Ingest Protocol, was developed to solve the biggest pain point with adopting WebRTC as a serious, professional, robust contribution protocol: Media Ingest.<br><br>WHIP enables WebRTC to retain its technical advantages over older protocols like RTMP when it comes to resiliency over bad network conditions, adaptability, end-to-end encryption, and new codec support (hello AV1 SVC).<br><br>It also removes the barrier WebRTC had with a lack of standard signaling protocol that has made it hard to support as a software solution, and difficult for hardware encoders to implement WebRTC.<br><br>Developers love WebRTC because it is an IETF &amp; W3C standard that makes it easy to write client applications with native broadcast and playback support on billions of devices worldwide. And the WISH working group at the IETF is currently reviewing WHIP with a milestone to publish it as a standard by December 2021.<br><br>Implementing the open source WHIP library in your software or hardware encoder is all you need to support the entire WebRTC stack on the sender side.<br><br>It’s time to WHIP WebRTC into shape and take advantage of WebRTC end-to-end, as it was meant to be, natively on every device.",
    "speakers": [
      "Ryan Jespersen"
    ]
  },
  "the-dashboard-wont-load-tales-from-a-super-bowl-warzoom": {
    "title": "The dashboard won’t load! Tales from a Super Bowl WarZoom",
    "description": "As streaming exits its infancy stages and becomes a true contender to broadcast television, quality and reliability are under scrutiny. Beyond improving the resiliency of streaming architectures, it is imperative organizations operationalize the way streaming workflows are monitored if they wish to compete with the broadcast standard of five 9&#39;s reliability (99.999% uptime). <br><br>I would like to present a glance of how our team is solving this problem, the challenges faced and lessons learned from doing this at &quot;Super Bowl&quot; scale. <br>Solving the observability challenge requires:<br><br>1. Balancing depth and breadth of data: understanding what data is and isn&#39;t necessary, choosing when to sample, and knowing when to collect raw logs as opposed to aggregate metrics.<br>2. Architecting a scaleable data platform which does not sacrifice data integrity for ingestion speed.<br>3. Normalizing metrics across vendors (especially CDNs.)<br>4. Tracking the bit-flow of a stream from acquisition through playback -- having the correct metadata and tags to support a primary key across workflow components. <br>5. Automating provisioning data, and exposing the data via a scaleable API.<br><br>Throughout this process we have learned a great deal about our streaming workflows (and optimized a few systems as a result), data engineering and building user-friendly APIs. It would be honored to share our learnings with the industry.<br><br>This talk is primarily intended for: streaming data engineers, streaming operations, and those managing CDN integrations.`",
    "speakers": [
      "Sean McCarthy"
    ]
  },
  "whats-your-glitch-an-ode-to-datamoshing": {
    "title": "What&#39;s your glitch?: An ode to datamoshing",
    "description": "It&#39;s not a new trick, but I still enjoy a deliberately glitched video. It evokes nostalgia of the early days of video on the web, when the glitches were unintentional.  <br><br>This talk will go over what datamoshing is and the different types thereof. It will succinctly explain motion vectors and the frames, both I and P, as well as answer the question, What is the AVI container and why is it used in datamoshing? There will be a slide with links to resources to make one&#39;s own glitched video, but the talk will focus on a lower level explanation of why and how datamoshing occurs rather than a tutorial on how to accomplish it. And of course, there will be a few fun examples sprinkled in there.<br>Datamoshing is a fun throwback aesthetic that uses some video encoding techniques we mostly take for granted in the year 2021. I think it&#39;s time to revisit it, appreciate the &quot;misuse&quot;, and maybe learn what a RIFF file is while we&#39;re at it.<br><br>",
    "speakers": [
      "Vanessa Pyne"
    ]
  },
  "origins-of-many-strange-things-in-video": {
    "title": "Origins of many strange things in video",
    "description": "In this talk I will review some odd-looking numbers and design aspects that exists in modern days video and media systems and try to explain how and why they have been derived, what was the original intended utility and why we have stuck with them. <br>Among things I will review will be:<br>Interlace scan -- 1880 patent by Maurice Leblanc: reducing bandwidth for line-by-line transmission of 2D images<br>YUV color spaces -- 1938 patent by Georges Valensi: compatibility with Black&amp;White TV<br>4:2:0 choma subsampling: 1949 patent by Alda Bedford, RCA; reducing bandwidth<br>First color TV system: 1951: CBS field-sequential system; did not use YUV!<br>First HDTV System: 1979 Japanese MUSE system. Analog. Extreme form of interlace and subsamling of everything. <br>3:2 pulldowns -- 1950s “Flying Spot” machines (all analog, this was well before CCDs!)<br>25/30 fps framerates -- 1930s: use of 50/60Hz AC power frequency as base reference<br>29.97 framerate -- 1953: bandwidth constraints in the design of NTSC <br>44.1kHz sampling rate -- 1980: desire to fit Beethoven&#39;s 9th Symphony on a single CD<br>NTSC, EBU, and SMPTE-C colors -- a case when industry has ignored the standard<br>Standard resolutions: 1080p, 720p, 480i, 576i, etc. -- each has history!<br>Anamorthic formats: 10:11, 12:11, 40:33, 4:3, 3:2, 64:33, etc. - each has history!<br>This could be a fun short 10min talk.  If desired, I can expand it to 20 min and offer more details. ",
    "speakers": [
      "Yuriy Reznik"
    ]
  },
  "measuring-the-live-video-latency-at-scale": {
    "title": "Measuring the live video latency at scale",
    "description": "Imagining a live streaming viewer wants to know the delay of their live video, or a live infra engineer needs to capture the latency performance of their pipeline, how could you have the latency systematically measured nearline in large scale?  <br> <br>In this talk, we’d like to present a latency measurement framework that we developed for Linkedin live video. It supports capturing the per stream latency from ingestion to playback, leveraging technologies including media signaling, header manipulation, and streaming processing, etc.   <br> <br>We’ll be covering 4 major latency contributing factors in the live pipeline: ingestion to storage, origin server processing, origin to CDN client, and client player latency. We will discuss how we work across the stack with Azure Media Service to capture each latency factor at per client level in nearline fashion. We’ll also share some “glitches” we caught while building the flow. ",
    "speakers": [
      "Yurong Jiang"
    ]
  },
  "whats-new-in-hls": {
    "title": "What&#39;s new in HLS",
    "description": "This talk will cover the new features being added to the HLS spec. In particular Content Steering and Interstitials.<br><br>We will cover what the challenges have been in the past for Multi-CDN Steering and SSAI and how these systems have been built in the past. We will then go into how these new features are supported in HLS and what they enable/make easier. <br><br>Finally we will look at how you can start implementing these new features and talk about what it will mean for support across non-Apple devices.",
    "speakers": [
      "Zac Shenker"
    ]
  }
}
